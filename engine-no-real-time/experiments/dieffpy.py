import diefpy
import pandas as pd  # for displaying the data in a nice way
import matplotlib.pyplot as plt
import sys
import os
import re


#########################################################################################
# edit: to show approaches listed respecting numerical order in the plots
# OTHERWISE: comment these lines and use the diefpy corresponding function call
import matplotlib.lines as mlines
import matplotlib.ticker as mticker
import numpy as np
from matplotlib.figure import Figure

from diefpy.radaraxes import radar_factory


DEFAULT_COLORS = ("#ECC30B", "#D56062", "#84BCDA")
"""Default colors for printing plots: yellow, red, blue"""

COLORS = (
    "#ECC30B",  # Yellow
    "#D56062",  # Coral
    "#84BCDA",  # Light Blue
    "#F5A623",  # Orange
    "#7ED321",  # Green
    "#4A90E2",  # Blue
    "#9013FE",  # Purple
    "#50E3C2",  # Teal
    "#F8E71C",  # Light Yellow
    "#B8E986",  # Light Green
    "#D0021B",  # Red
    "#8B572A",  # Brown
    "#9B9B9B",  # Gray
    "#F2C94C",  # Mustard
    "#E94E77",  # Pink
    "#56CCF2",  # Sky Blue
    "#FF477E",  # Hot Pink
    "#35A7FF",  # Sky Blue
    "#F9AFAE",  # Light Coral
    "#21C28E",  # Mint Green
    "#C8E8C1",  # Pale Green
    "#F1A7C7",  # Soft Pink
)

####################################################################################################################


def plot_answer_trace_edit(
    inputtrace: np.ndarray, inputtest: str, colors: list = DEFAULT_COLORS
) -> Figure:
    """
    Plots the answer trace of a given test for all approaches.

    Answer traces record the points in time when an approach produces an answer.
    The plot generated by this function shows the answer traces of all approaches
    for the same test, e.g., execution of a specific query.

    :param inputtrace: Dataframe with the answer trace. Attributes of the dataframe: test, approach, answer, time.
    :param inputtest: Specifies the specific test to analyze from the answer trace.
    :param colors: List of colors to use for the different approaches.
    :return: Plot of the answer traces of each approach when evaluating the input test.

    **Examples**

    >>> plot_answer_trace(traces, "Q9.sparql")
    >>> plot_answer_trace(traces, "Q9.sparql", ["#ECC30B","#D56062","#84BCDA"])
    """
    # Obtain test and approaches to compare.
    results = inputtrace[inputtrace["test"] == inputtest]
    approaches = np.unique(inputtrace["approach"])

    sorted_approaches = sorted(
        approaches,
        key=lambda x: [int(i) if i.isdigit() else i for i in re.split("([0-9]+)", x)],
    )

    color_map = dict(zip(approaches, colors))

    # Generate plot.
    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)
    for a in sorted_approaches:
        subtrace = results[results["approach"] == a]
        if subtrace.size == 0:
            continue
        plt.plot(
            subtrace["time"],
            subtrace["answer"],
            color=color_map[a],
            label=a,
            marker="o",
            markeredgewidth=0.0,
            linestyle="None",
        )

    plt.xlabel("Time")
    plt.ylabel("# Answers Produced")
    plt.legend(loc="upper left")
    plt.title(inputtest, fontsize=16, loc="center", pad=20)
    plt.tight_layout()

    return fig


def plot_performance_of_approaches_with_dieft_edit(
    allmetrics: np.ndarray, q: str, colors: list = DEFAULT_COLORS
) -> Figure:
    """
    Generates a radar plot that compares **dief@t** with conventional metrics for a specific test.

    This function plots the results reported for a single given test in "Experiment 1" (see :cite:p:`dief`).
    "Experiment 1" compares the performance of testing approaches when using metrics defined in the literature
    (*total execution time*, *time for the first tuple*, *throughput*, and *completeness*) and the metric **dieft@t**.

    :param allmetrics: Dataframe with all the metrics from "Experiment 1".
    :param q: ID of the selected test to plot.
    :param colors: List of colors to use for the different approaches.
    :return: Matplotlib radar plot for the specified test over the provided metrics.

    **Examples**

    >>> plot_performance_of_approaches_with_dieft(extended_metrics, "Q9.sparql")
    >>> plot_performance_of_approaches_with_dieft(extended_metrics, "Q9.sparql", ["#ECC30B","#D56062","#84BCDA"])
    """
    # Initialize output structure.
    df = np.empty(
        shape=0,
        dtype=[
            ("invtfft", allmetrics["invtfft"].dtype),
            ("invtotaltime", allmetrics["invtotaltime"].dtype),
            ("comp", float),
            ("throughput", allmetrics["throughput"].dtype),
            ("dieft", allmetrics["dieft"].dtype),
        ],
    )

    # Obtain approaches.
    approaches = np.unique(allmetrics["approach"])
    sorted_approaches = sorted(
        approaches,
        key=lambda x: [int(i) if i.isdigit() else i for i in re.split("([0-9]+)", x)],
    )
    color_map = dict(zip(approaches, colors))
    labels = []
    for a in sorted_approaches:
        submetric_approaches = allmetrics[
            (allmetrics["approach"] == a) & (allmetrics["test"] == q)
        ]

        if submetric_approaches.size == 0:
            continue
        else:
            labels.append(a)

        res = np.array(
            [
                (
                    (submetric_approaches["invtfft"]),
                    (submetric_approaches["invtotaltime"]),
                    (submetric_approaches["comp"]),
                    (submetric_approaches["throughput"]),
                    (submetric_approaches["dieft"]),
                )
            ],
            dtype=[
                ("invtfft", submetric_approaches["invtfft"].dtype),
                ("invtotaltime", submetric_approaches["invtotaltime"].dtype),
                ("comp", float),
                ("throughput", submetric_approaches["throughput"].dtype),
                ("dieft", submetric_approaches["dieft"].dtype),
            ],
        )
        df = np.append(df, res, axis=0)

    # Get maximum values
    maxs = [
        df["invtfft"].max(),
        df["invtotaltime"].max(),
        df["comp"].max(),
        df["throughput"].max(),
        df["dieft"].max(),
    ]

    # Normalize the data
    for row in df:
        row["invtfft"] = row["invtfft"] / maxs[0]
        row["invtotaltime"] = row["invtotaltime"] / maxs[1]
        row["comp"] = row["comp"] / maxs[2]
        row["throughput"] = row["throughput"] / maxs[3]
        row["dieft"] = row["dieft"] / maxs[4]

    # Plot metrics using spider plot.
    df = df.tolist()
    N = len(df[0])
    theta = radar_factory(N, frame="polygon")
    spoke_labels = ["(TFFT)^-1", "(ET)^-1       ", "Comp", "T", "     dief@t"]
    case_data = df
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection="radar"))
    fig.subplots_adjust(top=0.85, bottom=0.05)
    ax.set_ylim(0, 1)
    ticks_loc = ax.get_yticks()
    ax.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))
    ax.set_yticklabels("" for _ in ticks_loc)
    legend_handles = []
    for d, label in zip(case_data, labels):
        legend_handles.append(
            mlines.Line2D([], [], color=color_map[label], ls="-", label=label)
        )
        ax.plot(theta, d, label=label, color=color_map[label], zorder=10, clip_on=False)
        ax.fill(theta, d, label=label, facecolor=color_map[label], alpha=0.15)

    ax.set_varlabels(spoke_labels)
    ax.tick_params(labelsize=14)
    ax.legend(
        handles=legend_handles,
        loc=(0.80, 0.90),
        labelspacing=0.1,
        fontsize="medium",
        frameon=False,
    )

    plt.setp(ax.spines.values(), color="grey")
    plt.title(q, fontsize=16, loc="center", pad=30)
    plt.tight_layout()

    return fig


def plot_continuous_efficiency_with_diefk_edit(
    diefkDF: np.ndarray, q: str, colors: list = DEFAULT_COLORS
) -> Figure:
    """
    Generates a radar plot that compares **dief@k** at different answer completeness percentages for a specific test.

    This function plots the results reported for a single given test in "Experiment 2"
    (see :cite:p:`dief`).
    "Experiment 2" measures the continuous efficiency of approaches when producing
    the first 25%, 50%, 75%, and 100% of the answers.

    :param diefkDF: Dataframe with the results from "Experiment 2".
    :param q: ID of the selected test to plot.
    :param colors: List of colors to use for the different approaches.
    :return: Matplotlib plot for the specified test over the provided metrics.

    **Examples**

    >>> plot_continuous_efficiency_with_diefk(diefkDF, "Q9.sparql")
    >>> plot_continuous_efficiency_with_diefk(diefkDF, "Q9.sparql", ["#ECC30B","#D56062","#84BCDA"])
    """
    # Initialize output structure.
    df = np.empty(
        shape=0,
        dtype=[
            ("diefk25", float),
            ("diefk50", float),
            ("diefk75", float),
            ("diefk100", float),
        ],
    )

    # Obtain approaches.
    approaches = np.unique(diefkDF["approach"])
    sorted_approaches = sorted(
        approaches,
        key=lambda x: [int(i) if i.isdigit() else i for i in re.split("([0-9]+)", x)],
    )
    labels = []
    color_map = dict(zip(approaches, colors))

    for a in sorted_approaches:
        submetric_approaches = diefkDF[
            (diefkDF["approach"] == a) & (diefkDF["test"] == q)
        ]

        if submetric_approaches.size == 0:
            continue
        else:
            labels.append(a)

        res = np.array(
            [
                (
                    (submetric_approaches["diefk25"]),
                    (submetric_approaches["diefk50"]),
                    (submetric_approaches["diefk75"]),
                    (submetric_approaches["diefk100"]),
                )
            ],
            dtype=[
                ("diefk25", submetric_approaches["diefk25"].dtype),
                ("diefk50", submetric_approaches["diefk50"].dtype),
                ("diefk75", submetric_approaches["diefk75"].dtype),
                ("diefk100", submetric_approaches["diefk100"].dtype),
            ],
        )

        df = np.append(df, res, axis=0)

    # Get maximum values
    maxs = [
        df["diefk25"].max(),
        df["diefk50"].max(),
        df["diefk75"].max(),
        df["diefk100"].max(),
    ]

    # Normalize the data
    for row in df:
        row["diefk25"] = row["diefk25"] / maxs[0]
        row["diefk50"] = row["diefk50"] / maxs[1]
        row["diefk75"] = row["diefk75"] / maxs[2]
        row["diefk100"] = row["diefk100"] / maxs[3]

    # Plot metrics using spider plot.
    df = df.tolist()
    N = len(df[0])
    theta = radar_factory(N, frame="polygon")
    spoke_labels = ["k=25%", "k=50%      ", "k=75%", "        k=100%"]
    case_data = df
    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(projection="radar"))
    fig.subplots_adjust(top=0.85, bottom=0.05)
    ax.set_ylim(0, 1)
    ticks_loc = ax.get_yticks()
    ax.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))
    ax.set_yticklabels("" for _ in ticks_loc)
    legend_handles = []
    for d, label in zip(case_data, labels):
        legend_handles.append(
            mlines.Line2D([], [], color=color_map[label], ls="-", label=label)
        )
        ax.plot(theta, d, color=color_map[label], zorder=10, clip_on=False)
        ax.fill(theta, d, facecolor=color_map[label], alpha=0.15)
    ax.set_varlabels(spoke_labels)
    ax.tick_params(labelsize=14, zorder=0)

    ax.legend(
        handles=legend_handles,
        loc=(0.80, 0.90),
        labelspacing=0.1,
        fontsize="medium",
        frameon=False,
    )

    plt.setp(ax.spines.values(), color="grey")
    plt.title(q, fontsize=16, loc="center", pad=30)
    plt.tight_layout()

    return fig


####################################################################################################################


if len(sys.argv) < 3:
    print("Error, run like: $>python dieffpy.py resultsDirectoryPath TEST(name)")
    exit(1)

# Read name of the directory
input_dir = sys.argv[1]
test_name = sys.argv[2]

metrics_all = []
header_metrics = False
traces_all = []
header_traces = False

subdirs = [
    d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))
]

# Sort subdirectories alphanumerically, respecting numerical order
sorted_subdirs = sorted(
    subdirs,
    key=lambda x: [int(i) if i.isdigit() else i for i in re.split("([0-9]+)", x)],
)


# Subdirectories
for subdir in sorted_subdirs:
    if os.path.isdir(os.path.join(input_dir, subdir)):
        # print(subdir)
        metrics_file = os.path.join(input_dir, subdir, "metrics.csv")
        # print(metrics_file)

        if os.path.isfile(metrics_file):  # Check if 'metrics.csv' exists
            try:
                # Read the CSV file
                if not header_metrics:
                    # Read with the header for the first file
                    df = pd.read_csv(metrics_file)
                    header_metrics = True
                else:
                    # Skip the header for subsequent files
                    df = pd.read_csv(metrics_file, header=0)

                # print(df)
                metrics_all.append(df)
                # print(metrics_all)
            except Exception as e:
                print(f"Error reading {metrics_file}: {e}")

        traces_file = os.path.join(input_dir, subdir, "trace.csv")
        # print(traces_file)
        if os.path.isfile(traces_file):
            try:
                # Read the CSV file
                if not header_traces:
                    # Read with the header for the first file
                    df = pd.read_csv(traces_file)
                    header_traces = True
                else:
                    # Skip the header for subsequent files
                    df = pd.read_csv(traces_file, header=0)

                traces_all.append(df)
            except Exception as e:
                print(f"Error reading {traces_file}: {e}")


if metrics_all:
    metrics_all_df = pd.concat(metrics_all, ignore_index=True)
    # output metrics all csv
    output_metrics = input_dir + "/metrics.csv"
    metrics_all_df.to_csv(output_metrics, index=False, header=header_metrics)
else:
    print("No metrics.csv files found to combine.")


if traces_all:
    traces_all_df = pd.concat(traces_all, ignore_index=True)
    # output metrics all csv
    output_traces = input_dir + "/trace.csv"
    traces_all_df.to_csv(output_traces, index=False, header=header_traces)
else:
    print("No traces.csv files found to combine.")

############################ RESULT PLOTS AND METRICS ############################

outputPlotDir = input_dir + "/plots/"
if not os.path.exists(outputPlotDir):
    os.makedirs(outputPlotDir)

traces = diefpy.load_trace(input_dir + "/trace.csv")
# Plot the answer trace recorded in `traces` for query `Q9.sparql`
diefpy.plot_answer_trace(traces, test_name, COLORS)
plt.savefig(outputPlotDir + "traces.png")

# computing dief@t until the time unit 10s
# dt = diefpy.dieft(traces, test_name, 10)
# print(pd.DataFrame(dt).head())

# computing dief@t until the time unit when the slowest approach finalizes its execution
dt = diefpy.dieft(traces, test_name)
print("dief@t until the time unit when the slowest approach finalizes its execution")
print(pd.DataFrame(dt).head())
print("____________________________________________________________________________")
print()

# print(input_dir + "/metrics.csv")
metrics = diefpy.load_metrics(input_dir + "/metrics.csv")

# Execution time plot
diefpy.plot_execution_time(metrics, COLORS, log_scale=True)
plt.savefig(outputPlotDir + "execTime.png")


# Create all metrics from the `traces` and `metrics`
# computes the results reported in the previously mentioned experiment, i.e.,
# - dief@t
# - time to first tuple (tfft)
# - execution time (totaltime)
# - number of answers produced (comp?)
# - throughput (= comp/totaltime)
# - inverse time to first tuple (1/tfft)
# - inverse execution time (1/totaltime)
exp1 = diefpy.performance_of_approaches_with_dieft(traces, metrics)
print("Create all metrics from the traces and metrics")
print(pd.DataFrame(exp1[exp1["test"] == test_name]).head())
print("____________________________________________________________________________")
print()


# Create radar plot to compare the performance of the approaches with dief@t and other metrics.
# - Plot interpretation: Higher is better.
diefpy.plot_performance_of_approaches_with_dieft(exp1, test_name, COLORS)
plt.savefig(outputPlotDir + "radar-dieft.png")


# dief@k:s
# The metric dief@k measures the diefficiency of a query engine while producing
# the first k answers when executing a query. Intuitively, approaches that require
# a shorter period of time to produce a certain number of answers are more efficient
# dief@k interpretation: Lower is better

# dief@k producing the first 5 answers
print("dief@k producing the first 5 answers")
dk = diefpy.diefk(traces, test_name, 5)
print(pd.DataFrame(dk).head())
print("____________________________________________________________________________")
print()


# dief@k producing the first 10 answers
print("dief@k producing the first 10 answers")
dk = diefpy.diefk(traces, test_name, 10)
print(pd.DataFrame(dk).head())
print("____________________________________________________________________________")
print()


# producing 50% of the answers
print("# producing 50% of the answers")
dk = diefpy.diefk2(traces, test_name, 0.50)
print(pd.DataFrame(dk).head())
print("____________________________________________________________________________")
print()


# 4.6. Measuring dief@t at Different Answer Completeness Percentages
# compares the performance of the three variants when producing different
# answer completeness percentages (25%, 50%, 75%, 100%) using dief@k.

# method diefpy.continuous_efficiency_with_diefk computes the dief@k metric
# for the previously mentioned answer completeness percentages.
# Plot interpretation: Lower is better.
exp2 = diefpy.continuous_efficiency_with_diefk(traces)
diefpy.plot_continuous_efficiency_with_diefk(exp2, test_name, COLORS)
plt.savefig(outputPlotDir + "radar-diefk.png")
